namespace: rackspace

replicaCount: 1

image:
  repository: ghcr.io/rackerlabs/alert-proxy
  tag: latest
  pullPolicy: IfNotPresent

nodeSelector:
  node-role.kubernetes.io/worker: worker

service:
  type: ClusterIP
  port: 80
  targetPort: 8000

pvc:
  enabled: true
  name: alert-proxy-volume-claim
  accessModes:
    - ReadWriteOnce
  storage: 20Gi

# These secrets must exist in the target namespace before deploying the chart.
# The 'name' refers to the Kubernetes Secret object name.
# The 'key' refers to the specific data key within that Secret.
secrets:
secrets:
  coreAccountId:
    name: "core-account-id-secret" # Name of the existing Kubernetes Secret
    key: "core_account_number"     # Key within that Secret
  overseerDeviceId:
    name: "overseer-core-device-id-secret"
    key: "overseer_core_device_id"
  accountServiceToken:
    name: "account-service-token-secret"
    key: "account_service_token"
  alertManagerBaseUrl:
    name: "alert-manager-base-url-secret"
    key: "alert_manager_base_url"

config:
  debug: false
  testing: false
  app_debug: false
  logging:
    log_level: 'INFO'
    log_dir: 'logs'
    log_file_name: 'alert_proxy.log'
  alert_proxy_config:
    log_level: 'INFO'
    alert_verification: true
    # core_account_id, core_overseer_id, account_secret,
    # am_v2_base_url are now expected to be provided via environment variables,
    # which are sourced from existing Kubernetes Secrets.

livenessProbe:
  httpGet:
    path: /
    port: 80
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3
readinessProbe:
  httpGet:
    path: /
    port: 80
  initialDelaySeconds: 15
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

serviceAccount: {}
podAnnotations: {}
podLabels: {}
podSecurityContext: {}
securityContext: {}
ingress: {}
resources: {}
autoscaling: {}
volumes: []
volumeMounts: []
tolerations: []
affinity: {}
